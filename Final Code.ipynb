{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpXFq9nkK4bX",
        "outputId": "14a3ddb2-9623-43cc-cf9d-0bc326a065e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bAumjtyjJyRA",
        "outputId": "f7c0a46a-845a-4662-cb26-9b51c3a54f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -qq accelerate tensorboard transformers ftfy gradio\n",
        "!pip install -qq \"ipywidgets>=7,<8\"\n",
        "!pip install -qq bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5xZVqyWJyRI",
        "outputId": "96f4d0f3-ca2b-48ae-9abf-d062df772721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Your GPU may not be supported for xformers precompiled wheels.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --pre triton\n",
        "\n",
        "from subprocess import getoutput\n",
        "import time\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "    gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "    gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "    gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "    gpu = 'A100'\n",
        "else:\n",
        "    gpu = None\n",
        "    print(\"Your GPU may not be supported for xformers precompiled wheels.\")\n",
        "\n",
        "if gpu == 'T4':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif gpu == 'P100':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif gpu == 'V100':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif gpu == 'A100':\n",
        "    %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O384EDXpJyRM",
        "outputId": "a0ad0f2b-3c6c-419a-f827-57b382387ba3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:bitsandbytes.cextension:Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
            "    lib = get_native_library()\n",
            "          ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
            "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n",
            "    return self._dlltype(name)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cpu.so)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from accelerate import Accelerator, notebook_launcher, logging as accel_logging, utils as accel_utils\n",
        "from accelerate.utils import set_seed\n",
        "\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from diffusers.optimization import get_scheduler\n",
        "\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import bitsandbytes as bnb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZT9tTYKRzgL",
        "outputId": "2a3b5504-74c1-4550-8415-222cae82fa03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMG_20230423_082747.jpg    IMG_20240528_105934.jpg  IMG_20240903_062903.jpg\n",
            "IMG_20230423_083415.jpg    IMG_20240531_014610.jpg  IMG_20240911_063611.jpg\n",
            "IMG_20230901_095400.jpg    IMG_20240712_092230.jpg  IMG_20240911_063615.jpg\n",
            "IMG_20230901_095403.jpg    IMG_20240712_092238.jpg  IMG_20240911_063636.jpg\n",
            "IMG_20231101_172932.jpg    IMG_20240712_092244.jpg  IMG_20240911_063732.jpg\n",
            "IMG_20240303_070457~3.jpg  IMG_20240825_113037.jpg  IMG_20240911_063840.jpg\n",
            "IMG_20240308_072035.jpg    IMG_20240901_163621.jpg\n",
            "IMG_20240528_105928.jpg    IMG_20240901_163641.jpg\n"
          ]
        }
      ],
      "source": [
        "!dir \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SeKTbhKIJyRO"
      },
      "outputs": [],
      "source": [
        "# The pretrained Stable Diffusion model to start from\n",
        "pretrained_model_name_or_path = r\"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/model/\"\n",
        "\n",
        "# Unique prompt identifier for your pet dog Bruno\n",
        "instance_prompt = \"<bruno>\"\n",
        "\n",
        "# Path to your custom dataset of Bruno's images (ensure this folder exists)\n",
        "instance_data_dir = r\"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/\"\n",
        "\n",
        "# Option for prior preservation (set to False if you only train on Bruno)\n",
        "with_prior_preservation = False\n",
        "prior_loss_weight = 1.0  # adjust if using prior preservation\n",
        "\n",
        "# If using prior preservation, you can specify a class prompt and folder for generic dog images:\n",
        "class_prompt = \"a photo of a dog <bruno>\"\n",
        "class_data_dir = r\"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/class_images\"  # make sure this folder exists (or let the code create it)\n",
        "num_class_images = 12  # number of class images to generate/use\n",
        "\n",
        "# Training hyperparameters\n",
        "learning_rate = 5e-6\n",
        "max_train_steps = 300  # adjust based on dataset size and experimentation\n",
        "train_batch_size = 2\n",
        "gradient_accumulation_steps = 2\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Mixed precision and gradient checkpointing settings\n",
        "mixed_precision = \"fp16\"  # or \"bf16\" if supported\n",
        "gradient_checkpointing = True\n",
        "use_8bit_adam = True\n",
        "\n",
        "# Output directory to save checkpoints and the final model\n",
        "output_dir = \"dreambooth-leo\"\n",
        "seed = 3434554\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9f0IJBI5JyRP"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = Namespace(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    resolution=512,  # Stable Diffusion typically uses 512x512 images\n",
        "    center_crop=True,\n",
        "    train_text_encoder=False,\n",
        "    instance_data_dir=instance_data_dir,\n",
        "    instance_prompt=instance_prompt,\n",
        "    learning_rate=learning_rate,\n",
        "    max_train_steps=max_train_steps,\n",
        "    save_steps=50,\n",
        "    train_batch_size=train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    mixed_precision=mixed_precision,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    use_8bit_adam=use_8bit_adam,\n",
        "    seed=seed,\n",
        "    with_prior_preservation=with_prior_preservation,\n",
        "    prior_loss_weight=prior_loss_weight,\n",
        "    sample_batch_size=train_batch_size,  # For generating class images if needed\n",
        "    class_data_dir=class_data_dir,\n",
        "    class_prompt=class_prompt,\n",
        "    num_class_images=num_class_images,\n",
        "    lr_scheduler=\"constant\",\n",
        "    lr_warmup_steps=100,\n",
        "    output_dir=output_dir,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZQhuaZsyJyRQ"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "class DreamBoothDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        instance_data_root,\n",
        "        instance_prompt,\n",
        "        tokenizer,\n",
        "        class_data_root=None,\n",
        "        class_prompt=None,\n",
        "        size=512,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.instance_data_root = Path(instance_data_root)\n",
        "        if not self.instance_data_root.exists():\n",
        "            raise ValueError(\"Instance images root doesn't exist.\")\n",
        "\n",
        "        self.instance_images_path = list(self.instance_data_root.iterdir())\n",
        "        self.num_instance_images = len(self.instance_images_path)\n",
        "        self.instance_prompt = instance_prompt\n",
        "        self._length = self.num_instance_images\n",
        "\n",
        "        # Setup class images if prior preservation is enabled\n",
        "        if class_data_root is not None:\n",
        "            self.class_data_root = Path(class_data_root)\n",
        "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
        "            self.class_images_path = list(self.class_data_root.iterdir())\n",
        "            self.num_class_images = len(self.class_images_path)\n",
        "            self._length = max(self.num_instance_images, self.num_class_images)\n",
        "            self.class_prompt = class_prompt\n",
        "        else:\n",
        "            self.class_data_root = None\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                # Modified normalization to account for RGB images:\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = {}\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        if instance_image.mode != \"RGB\":\n",
        "            instance_image = instance_image.convert(\"RGB\")\n",
        "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
        "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
        "            self.instance_prompt,\n",
        "            padding=\"do_not_pad\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "        ).input_ids\n",
        "\n",
        "        if self.class_data_root:\n",
        "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
        "            if class_image.mode != \"RGB\":\n",
        "                class_image = class_image.convert(\"RGB\")\n",
        "            example[\"class_images\"] = self.image_transforms(class_image)\n",
        "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
        "                self.class_prompt,\n",
        "                padding=\"do_not_pad\",\n",
        "                truncation=True,\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "            ).input_ids\n",
        "\n",
        "        return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T9KUAEDcJyRR"
      },
      "outputs": [],
      "source": [
        "def training_function(text_encoder, vae, unet):\n",
        "    logger = accel_logging.get_logger(__name__)\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        mixed_precision=args.mixed_precision,\n",
        "    )\n",
        "\n",
        "    # Freeze VAE parameters; optionally freeze the text encoder as well.\n",
        "    vae.requires_grad_(False)\n",
        "    if not args.train_text_encoder:\n",
        "        text_encoder.requires_grad_(False)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "        unet.enable_gradient_checkpointing()\n",
        "        if args.train_text_encoder:\n",
        "            text_encoder.gradient_checkpointing_enable()\n",
        "\n",
        "    # Use 8-bit Adam if enabled.\n",
        "    optimizer_class = bnb.optim.AdamW8bit if args.use_8bit_adam else torch.optim.AdamW\n",
        "    params_to_optimize = (\n",
        "        itertools.chain(unet.parameters(), text_encoder.parameters())\n",
        "        if args.train_text_encoder else unet.parameters()\n",
        "    )\n",
        "    optimizer = optimizer_class(params_to_optimize, lr=args.learning_rate)\n",
        "\n",
        "    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "    # Load the tokenizer (used for prompt encoding)\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
        "\n",
        "    # Create the dataset and dataloader.\n",
        "    train_dataset = DreamBoothDataset(\n",
        "        instance_data_root=args.instance_data_dir,\n",
        "        instance_prompt=args.instance_prompt,\n",
        "        tokenizer=tokenizer,\n",
        "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
        "        class_prompt=args.class_prompt,\n",
        "        size=args.resolution,\n",
        "        center_crop=args.center_crop,\n",
        "    )\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
        "        if args.with_prior_preservation:\n",
        "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
        "            pixel_values += [example[\"class_images\"] for example in examples]\n",
        "\n",
        "        pixel_values = torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()\n",
        "        input_ids = tokenizer.pad(\n",
        "            {\"input_ids\": input_ids},\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length\n",
        "        ).input_ids\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"pixel_values\": pixel_values}\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
        "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    if args.train_text_encoder:\n",
        "        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "    else:\n",
        "        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            unet, optimizer, train_dataloader, lr_scheduler\n",
        "        )\n",
        "\n",
        "    # Prepare VAE, text encoder for inference (cast to proper dtype)\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.decoder.to(\"cpu\")\n",
        "    if not args.train_text_encoder:\n",
        "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w/ accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "\n",
        "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        unet.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Encode images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
        "                latents = latents * 0.18215\n",
        "\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get text embeddings for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "                # Determine loss target based on prediction type\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                if args.with_prior_preservation:\n",
        "                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
        "                    target, target_prior = torch.chunk(target, 2, dim=0)\n",
        "                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "                    prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
        "                    loss = loss + args.prior_loss_weight * prior_loss\n",
        "                else:\n",
        "                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "\n",
        "                if global_step % args.save_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "                        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                            args.pretrained_model_name_or_path,\n",
        "                            unet=accelerator.unwrap_model(unet),\n",
        "                            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "                        )\n",
        "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "                        pipeline.save_pretrained(save_path)\n",
        "\n",
        "            progress_bar.set_postfix({\"loss\": loss.detach().item()})\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "        accelerator.wait_for_everyone()\n",
        "        if global_step >= args.max_train_steps:\n",
        "            break\n",
        "\n",
        "    # Save the final model\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            args.pretrained_model_name_or_path,\n",
        "            unet=accelerator.unwrap_model(unet),\n",
        "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uFAuhURfJyRU"
      },
      "outputs": [],
      "source": [
        "# Load base models (text encoder, VAE, and U-Net)\n",
        "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
        "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
        "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4yI3CIyMTQT"
      },
      "outputs": [],
      "source": [
        "# Launch training with Accelerate\n",
        "notebook_launcher(training_function, args=(text_encoder, vae, unet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umlIPSxTnbFj"
      },
      "outputs": [],
      "source": [
        "# !cp -r \"dreambooth-leo\" \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/model/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U_NqQSyJyRV"
      },
      "outputs": [],
      "source": [
        "from diffusers import DPMSolverMultistepScheduler\n",
        "\n",
        "scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
        "    pretrained_model_name_or_path,  # original model path with scheduler config\n",
        "    subfolder=\"scheduler\"\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    args.output_dir,  # your fine-tuned model directory\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "283cMoj_JyRX"
      },
      "outputs": [],
      "source": [
        "def inference(prompt, num_samples=1, num_inference_steps=100, guidance_scale=20):\n",
        "    images = pipe(prompt, num_images_per_prompt=num_samples, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).images\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1PwKugqMOZe"
      },
      "outputs": [],
      "source": [
        "# Test with your custom token\n",
        "prompt = \"a photo of <bruno> in the park\"\n",
        "generated_images = inference(prompt, num_samples=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_b8TT_IMQv5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Example: Display multiple images in a loop\n",
        "for img in generated_images:\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")  # Hide axes\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ls4v6gxz4j7I"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YlBZJQ-AGhd"
      },
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "!pip install transformers torch torchvision diffusers accelerate\n",
        "!pip install scikit-image\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from skimage.metrics import structural_similarity\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load CLIP model for evaluation\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Preprocessing for CLIP model\n",
        "clip_preprocess = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "              std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n",
        "# Function to calculate CLIP score\n",
        "def calculate_clip_score(image, prompt):\n",
        "    inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    return probs[0][0].item() * 100  # Convert to percentage\n",
        "\n",
        "# Function to calculate SSIM score\n",
        "def calculate_ssim(real_image, generated_image):\n",
        "    real_np = np.array(real_image.resize((512, 512))).astype(np.float32) / 255.0\n",
        "    gen_np = np.array(generated_image.resize((512, 512))).astype(np.float32) / 255.0\n",
        "    ssim_score = structural_similarity(real_np, gen_np, channel_axis=-1, data_range=1.0)\n",
        "    return ssim_score * 100  # percentage for clarity\n",
        "\n",
        "# Main Evaluation function\n",
        "def evaluate_model(prompts, real_images_paths, pipe):\n",
        "    clip_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    for idx, prompt in enumerate(prompts):\n",
        "        # Generate image from prompt\n",
        "        generated_image = pipe(prompt=prompt, num_inference_steps=100, guidance_scale=20).images[0]\n",
        "\n",
        "        # Load corresponding real image\n",
        "        real_image = Image.open(real_images_paths[idx]).convert(\"RGB\")\n",
        "\n",
        "        # Calculate CLIP score\n",
        "        clip_score = calculate_clip_score(generated_image, prompt)\n",
        "        clip_scores.append(clip_score)\n",
        "\n",
        "        # Calculate SSIM score\n",
        "        ssim_score = calculate_ssim(real_image, generated_image)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Display scores\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"CLIP Score: {clip_score:.2f}%\")\n",
        "        print(f\"SSIM Score: {ssim_score:.2f}%\\n\")\n",
        "\n",
        "        # Optional: Display generated vs real image\n",
        "        print(\"Generated vs Real Image:\")\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(generated_image)\n",
        "        axes[0].set_title(\"Generated Image\")\n",
        "        axes[0].axis(\"off\")\n",
        "        axes[1].imshow(real_image)\n",
        "        axes[1].set_title(\"Real Image\")\n",
        "        axes[1].axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    # Average scores\n",
        "    print(f\"\\nAverage CLIP Score: {np.mean(clip_scores):.2f}%\")\n",
        "    print(f\"Average SSIM Score: {np.mean(ssim_scores):.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amYVNnn197SE"
      },
      "outputs": [],
      "source": [
        "# Gather all caption entries into lists\n",
        "test_prompts = [\n",
        "    \"<bruno> sitting happily with his tongue out, looking cheerful and excited. The bright expression on his face and his slightly tilted head give him an adorable and friendly vibe.\",\n",
        "    \"<bruno> lying comfortably on a green platform, looking up with his tongue out in a joyful and relaxed manner. His soft golden fur contrasts beautifully with the background.\",\n",
        "    \"<bruno> sitting inside his playpen, full of energy and excitement. His bright eyes and wagging tail suggest he is happy and eager for attention.\",\n",
        "    \"<bruno> lying on the ground with a big happy grin, enjoying the moment as he gets a gentle pat on his chin. His playful and affectionate nature shines through in this close-up shot.\",\n",
        "    \"<bruno> looking up with excitement and love, his mouth open in a playful and joyful expression. His ears perked up show he is attentive and enjoying the interaction.\",\n",
        "    \"<bruno> lying on the floor in a relaxed state, gazing calmly at the camera. His peaceful expression and stretched-out posture suggest he's winding down after a long day.\",\n",
        "    \"<bruno> enjoying the outdoors, lying on patterned pavement with a bright expression. His tongue hanging out and the silver chain around his neck add to his playful and confident look.\",\n",
        "    \"<bruno> standing with his front paws on the bed, eagerly looking at the camera with his tongue out. His eyes are filled with excitement, and the bookshelf in the background adds a cozy indoor setting.\",\n",
        "    \"<bruno> sitting on a rough outdoor surface, looking up curiously with his mouth slightly open. His slightly dusty fur and bright eyes show that he's been having an adventurous time outside.\",\n",
        "    \"<bruno> sitting attentively, gazing upwards with an excited expression. His playful energy is evident as he tilts his head slightly, his tongue peeking out while he enjoys the outdoor breeze.\",\n",
        "    \"<bruno> lounging on a beautifully tiled outdoor patio, basking in the warm sunlight. His tongue is out in a happy and relaxed expression, while the greenery and house in the background add to the cozy, homey vibe.\",\n",
        "    \"<bruno> lying on a patterned outdoor tile floor, looking cheerful with his tongue out. The bright morning sunlight and lush greenery in the background create a perfect setting for his playful and content expression.\",\n",
        "    \"<bruno> happily lounging on the outdoor patio, his tongue hanging out as he enjoys the fresh air. The vibrant greenery and sunlit background create a perfect contrast to his relaxed yet playful posture.\",\n",
        "    \"<bruno> resting on a patterned tiled floor near a bench, looking relaxed yet alert. His tongue is out, showing his happy and content mood, while the decorative window grille in the background adds a unique architectural touch to the scene.\",\n",
        "    \"<bruno> lying on a cool marble floor, happily panting with his tongue out. His bright eyes and playful expression make it clear he's enjoying a relaxed moment indoors, with a blue cupboard in the background adding contrast to his light fur.\",\n",
        "    \"<bruno> lying comfortably on the cool marble floor, his tongue out as he pants happily. The bright lighting highlights his soft golden fur, and the deep blue cupboard in the background adds a striking contrast to his relaxed yet playful mood.\",\n",
        "    \"<bruno> lying down on the damp outdoor pavement, his eyes filled with sadness and exhaustion. His head rests heavily on the ground, and the misty morning setting, moss-covered walls, and distant houses add to the melancholic and lonely atmosphere of the scene.\",\n",
        "    \"<bruno> sitting on a patterned outdoor pavement, looking up with an innocent and slightly curious expression. His mouth is slightly open as if heâ€™s mid-whimper or about to bark, and the lush greenery with blooming flowers in the background adds a vibrant touch to the peaceful morning scene.\",\n",
        "    \"<bruno> sitting attentively on the outdoor pavement, gazing up with a curious yet serious expression. His slightly raised eyebrows and closed mouth give him a thoughtful look, while the lush green plants and vibrant flowers in the background add to the peaceful morning ambiance.\",\n",
        "    \"<bruno> sitting upright on the outdoor pavement, gazing upwards with a thoughtful and slightly longing expression. His soft eyes and the way his ears gently droop give him an endearing, almost wistful look, while the lush green plants and red flower pots add to the serene morning atmosphere.\",\n",
        "    \"<bruno> lying gracefully on the outdoor pavement, gazing into the distance with a calm and regal expression. His strong posture, paired with the lush greenery and blooming flowers in the background, gives him a noble and watchful presence in the peaceful morning setting.\",\n",
        "    \"<bruno> resting his head on the cool pavement, gazing into the distance with deep, soulful eyes. His expression carries a quiet sadness or longing, as if lost in thought. The lush green plants and red flower pots in the background add to the serene and contemplative mood of the image.\"\n",
        "]\n",
        "\n",
        "real_images_paths = [\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20231101_172932.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20230423_082747.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20230423_083415.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20230901_095400.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20230901_095403.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240531_014610.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240303_070457~3.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240308_072035.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240528_105928.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240528_105934.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240712_092230.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240712_092238.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240712_092244.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240825_113037.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240901_163621.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240901_163641.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240903_062903.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240911_063611.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240911_063615.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240911_063636.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240911_063732.jpg\",\n",
        "    \"/content/drive/MyDrive/Drexel/CS614 Applications of Machine Learning/CS614 Final Project/Dataset/Milan/IMG_20240911_063840.jpg\",\n",
        "]\n",
        "\n",
        "# Finally, call your evaluation function\n",
        "evaluate_model(test_prompts, real_images_paths, pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFsVqbpCiKHZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngOzRMPSiKL3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFLgkUZuiKSJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"INSERT API KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwPaD-IhiKUe",
        "outputId": "8e970c93-e3fe-4a9c-c34c-63407b786610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting backend.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile backend.py\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity\n",
        "\n",
        "class StableDiffusionBackend:\n",
        "    def __init__(self, model_path, device=\"cuda\"):\n",
        "        self.device = device\n",
        "        scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
        "            model_path, subfolder=\"scheduler\"\n",
        "        )\n",
        "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_path,\n",
        "            scheduler=scheduler,\n",
        "            torch_dtype=torch.float16\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    def generate_image(self, prompt, num_inference_steps=100, guidance_scale=7.5):\n",
        "        image = self.pipe(\n",
        "            prompt=prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale\n",
        "        ).images[0]\n",
        "        return image\n",
        "\n",
        "    def calculate_clip_score(self, image, prompt):\n",
        "        inputs = self.clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "        outputs = self.clip_model(**inputs)\n",
        "        logits = outputs.logits_per_image\n",
        "        probs = logits.softmax(dim=1)\n",
        "        return probs[0][0].item() * 100\n",
        "\n",
        "    def calculate_ssim(self, real_image, generated_image):\n",
        "        real_np = np.array(real_image.resize((512, 512))).astype(np.float32) / 255.0\n",
        "        gen_np = np.array(generated_image.resize((512, 512))).astype(np.float32) / 255.0\n",
        "        ssim_score = structural_similarity(real_np, gen_np, channel_axis=-1, data_range=1.0)\n",
        "        return ssim_score * 100\n",
        "\n",
        "    def evaluate(self, prompt, real_image_path):\n",
        "        generated_image = self.generate_image(prompt)\n",
        "        real_image = Image.open(real_image_path).convert(\"RGB\")\n",
        "        clip_score = self.calculate_clip_score(generated_image, prompt)\n",
        "        ssim_score = self.calculate_ssim(real_image, generated_image)\n",
        "\n",
        "        return {\n",
        "            'generated_image': generated_image,\n",
        "            'clip_score': clip_score,\n",
        "            'ssim_score': ssim_score\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkJze_VBs_G0",
        "outputId": "314fb7f5-381a-4109-a302-b3af5f951a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting therapist_agent.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile therapist_agent.py\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from backend import StableDiffusionBackend\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "class TherapistAgent:\n",
        "    def __init__(self, sd_backend: StableDiffusionBackend):\n",
        "        self.sd_backend = sd_backend\n",
        "        self.chat_system_prompt = (\n",
        "            \"You are a compassionate therapist chatbot helping users manage stress, anxiety, \"\n",
        "            \"or sadness through empathetic and concise supportive conversation.\"\n",
        "        )\n",
        "        self.image_system_prompt = (\n",
        "            \"You generate very short, simple, and clear prompts for image generation involving the user's pet (named <bruno>). \"\n",
        "            \"Examples: 'A photo of <bruno> playing in the garden', 'A cute picture of <bruno> sleeping.'\"\n",
        "        )\n",
        "\n",
        "    def chat(self, user_message):\n",
        "        # Generate empathetic text response\n",
        "        chat_messages = [\n",
        "            {\"role\": \"system\", \"content\": self.chat_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "\n",
        "        chat_response = client.chat.completions.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=chat_messages,\n",
        "            temperature=0.8\n",
        "        ).choices[0].message.content.strip()\n",
        "\n",
        "        # Separately generate short and simple image prompt\n",
        "        image_messages = [\n",
        "            {\"role\": \"system\", \"content\": self.image_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "\n",
        "        image_prompt_response = client.chat.completions.create(\n",
        "            model=\"gpt-4-turbo\",\n",
        "            messages=image_messages,\n",
        "            temperature=0.6,\n",
        "            max_tokens=25\n",
        "        ).choices[0].message.content.strip()\n",
        "\n",
        "        generated_image = self.sd_backend.generate_image(image_prompt_response)\n",
        "\n",
        "        return {\n",
        "            \"response_text\": chat_response,\n",
        "            \"image_prompt\": image_prompt_response,\n",
        "            \"generated_image\": generated_image\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uKYruSFjFQR",
        "outputId": "2637e55d-fb5f-416b-8196-5fe2dc66e178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from backend import StableDiffusionBackend\n",
        "from therapist_agent import TherapistAgent\n",
        "\n",
        "st.title(\"AI Therapist\")\n",
        "\n",
        "# Load therapist agent with cached resource\n",
        "@st.cache_resource\n",
        "def load_agent():\n",
        "    sd_backend = StableDiffusionBackend(model_path=\"/content/dreambooth-leo\")\n",
        "    return TherapistAgent(sd_backend)\n",
        "\n",
        "agent = load_agent()\n",
        "\n",
        "# User history input\n",
        "st.sidebar.header(\"User Information\")\n",
        "name = st.sidebar.text_input(\"Your Name\", value=\"Milan Varghese\")\n",
        "age = st.sidebar.number_input(\"Your Age\", min_value=1, max_value=100, value=25)\n",
        "occupation = st.sidebar.text_input(\"Occupation\", value=\"Master's Student in AI at Drexel University\")\n",
        "pet_keyword = st.sidebar.text_input(\"Pet Training Keyword\", value=\"<bruno>\")\n",
        "\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state[\"history\"] = []\n",
        "\n",
        "user_input = st.text_input(\"How are you feeling today?\")\n",
        "\n",
        "if st.button(\"Chat\"):\n",
        "    with st.spinner(\"Therapist is responding...\"):\n",
        "        # Combine user history into context string (concise for therapist, detailed for user)\n",
        "        user_context = (\n",
        "            f\"The user's name is {name}, a {age}-year-old {occupation}. \"\n",
        "            f\"The user's pet has been trained with the keyword {pet_keyword}.\"\n",
        "        )\n",
        "\n",
        "        # Therapist interaction (text response)\n",
        "        therapist_response = agent.chat(user_context + \" \" + user_input)\n",
        "\n",
        "    st.session_state[\"history\"].append({\"user\": user_input, \"therapist\": therapist_response})\n",
        "\n",
        "for exchange in reversed(st.session_state[\"history\"]):\n",
        "    st.write(f\"**You:** {exchange['user']}\")\n",
        "    st.write(f\"**Therapist:** {exchange['therapist']['response_text']}\")\n",
        "\n",
        "    if exchange['therapist']['generated_image']:\n",
        "        st.image(exchange['therapist']['generated_image'], caption=exchange['therapist']['image_prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAB8l5-v4w6R"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n",
        "\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66CCk4vOF9L",
        "outputId": "a87d80cd-8983-4179-ceb6-b085a83d894f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.91.219.19\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kyour url is: https://wet-foxes-dress.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd57-x0Sf20E",
        "outputId": "c70a19c8-59f5-417e-91c3-041566c93fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoint-100\tcheckpoint-200\tcheckpoint-300\tmodel_index.json  text_encoder\tunet\n",
            "checkpoint-150\tcheckpoint-250\tcheckpoint-50\tscheduler\t  tokenizer\tvae\n"
          ]
        }
      ],
      "source": [
        "!dir dreambooth-leo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFG-W42bt0G-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
